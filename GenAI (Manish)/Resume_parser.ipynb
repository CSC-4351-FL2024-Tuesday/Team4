{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLK5_F7D7spt",
        "outputId": "38eb3dac-dbf1-447e-9082-c1f82986feb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfminer\n",
            "  Downloading pdfminer-20191125.tar.gz (4.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pycryptodome (from pdfminer)\n",
            "  Downloading pycryptodome-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pdfminer\n",
            "  Building wheel for pdfminer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pdfminer: filename=pdfminer-20191125-py3-none-any.whl size=6140088 sha256=50e12b5c8387ba9beadfb387e8fc7e974611ceefda67c13231f0c1302e71e0f2\n",
            "  Stored in directory: /root/.cache/pip/wheels/4e/c1/68/f7bd0a8f514661f76b5cbe3b5f76e0033d79f1296012cbbf72\n",
            "Successfully built pdfminer\n",
            "Installing collected packages: pycryptodome, pdfminer\n",
            "Successfully installed pdfminer-20191125 pycryptodome-3.20.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pdfminer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JkIZl3Pt9jCC",
        "outputId": "2a5ee046-74b6-440c-9045-a4c5d23b5b6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfminer.six\n",
            "  Downloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (3.3.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (42.0.2)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.21)\n",
            "Installing collected packages: pdfminer.six\n",
            "Successfully installed pdfminer.six-20231228\n"
          ]
        }
      ],
      "source": [
        "!pip install pdfminer.six"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVvX4QhC8FRe",
        "outputId": "14b47647-a55b-488a-c497-86bee512f288"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement pdfminer.high_level (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for pdfminer.high_level\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "pip install pdfminer.high_level"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmCkSFovsBUO"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from pdfminer.high_level import extract_pages, extract_text_to_fp, extract_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWPd4JKnsMIv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b86fc554-d23a-4d56-fa5f-e627ff6b4c91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<LTTextBoxHorizontal(0) 201.001,742.986,406.296,781.275 'Manish Kolla \\nAtlanta, Georgia \\nmaneeshkolla2003@gmail.com, +1 (470)-647-9715 \\n'>\n",
            "<LTTextBoxHorizontal(1) 111.360,730.293,477.450,740.462 'LinkedIn: https://www.linkedin.com/in/manishkolla/ | GitHub: https://github.com/manishkolla \\n'>\n",
            "<LTTextBoxHorizontal(2) 6.480,622.622,545.764,720.819 \" EDUCATION AND HONORS \\n Georgia State University (Honors College) Atlanta, GA \\n Bachelor of Science in Computer Science, Certificate in Data Science \\n GPA: 3.88 \\nAwards: President's List (Spring 2022, Fall 2022, Fall 2023), Dean's List (Spring 2023, Fall 2021), US Presidential Tuition waiver \\nRelevant Coursework: Data Mining, Fundamentals of Data Science, Data Structures, System Level Programming, Design Analysis and \\nAlgorithms, Software Engineering, Programming Language Concepts, Machine Learning, Linear Algebra, Probability and Statistics. \\nCampus Involvement: Indian Student Organization (President), Association for Computing Machinery, Math and Stats Club \\n\">\n",
            "<LTTextBoxHorizontal(3) 519.620,695.102,582.716,705.062 'December 2024 \\n'>\n",
            "<LTTextBoxHorizontal(4) 0.960,601.059,191.643,612.099 '  WORK/INTERNSHIP EXPERIENCE \\n'>\n",
            "<LTTextBoxHorizontal(5) 6.480,573.418,270.809,594.573 '  Undergraduate Research Analyst/ Data Scientist \\n  Evidence Based Cybersecurity Research Group (EBCS), Atlanta, GA \\n'>\n",
            "<LTTextBoxHorizontal(6) 476.642,584.703,575.605,594.663 'September 2022- Present \\n'>\n",
            "<LTTextBoxHorizontal(7) 23.520,536.935,601.539,570.060 '•  Developing and implementing web automation framework to retrieve data from Clearnet, Darknet, and Telegram using Selenium and chrome/tor web \\ndrivers and debugging several web scrapers which boosted the workflow efficiency by 60% and accuracy up to 90% in scraping of web forums using \\nthe process of Extract-Transform- Load (ETL). \\n'>\n",
            "<LTTextBoxHorizontal(8) 23.520,501.895,602.519,535.020 '•  Leveraging Machine Learning, Deep Learning, and NLP to build models help identify patterns and trends in the data and using predictive analysis and \\ntraining neural networks to forecast the newly evolving catastrophes from the underground markets using Virtual Machines powered with Ubuntu and \\nPowerShell for advanced protection from malware threats from underground markets. \\n'>\n",
            "<LTTextBoxHorizontal(9) 23.520,489.902,603.916,499.980 '•  Effectively managing a team of 6 data scientists, fostering collaboration and ensuring project alignment. Presenting the findings and progress reports to \\n'>\n",
            "<LTTextBoxHorizontal(10) 41.520,478.379,244.166,488.339 'sponsors, contributing to successful project outcomes. \\n'>\n",
            "<LTTextBoxHorizontal(11) 0.960,449.222,244.766,471.182 '    STEM Tutor \\n      Department of Mathematics and Statistics, GSU, Atlanta, GA \\n'>\n",
            "<LTTextBoxHorizontal(12) 468.909,461.222,569.316,471.182 '       January 2022- Present \\n'>\n",
            "<LTTextBoxHorizontal(13) 18.960,408.180,602.270,447.156 '•  Assisting students with a class size of 40-50 on various kinds of math doubts and teaching them the topics they have difficulty understanding.  \\n• \\n• \\n'>\n",
            "<LTTextBoxHorizontal(14) 36.960,397.499,590.258,432.422 'Interacting with professor during the lecture halls and proctored during examinations organizing quizzes in absence of professors. \\nInteracting with the professor regularly and getting updates on the topic’s students have difficulty in understanding and organizing group \\nstudy sessions for those students. Tutored more than 500 happy students and received “Best Tutor Award for Academic Year Fall 2023.”  \\n'>\n",
            "<LTTextBoxHorizontal(15) 6.502,369.531,140.863,390.776 '   Information Technology Intern \\n   Lavner Education, Atlanta, GA \\n'>\n",
            "<LTTextBoxHorizontal(16) 482.531,380.816,578.993,390.776 ' June 2023- August 2023 \\n'>\n",
            "<LTTextBoxHorizontal(17) 18.960,355.140,601.413,367.356 '•  Maintenance of on-site computer hardware, including desktops, laptops, printers, peripherals, ensuring they are functioning optimally and up \\n'>\n",
            "<LTTextBoxHorizontal(18) 36.960,344.827,539.014,354.787 'to date for the students and managed the inventory of hardware and software assets and troubleshooting them when required. \\n'>\n",
            "<LTTextBoxHorizontal(19) 18.960,317.220,596.164,343.236 '•  Taught and graded assignments for a class size of 50 middle/high school students in Python, C, Machine Learning and Artificial Intelligence. \\n•  Debugged student codes, project backups, and computer clean-up to ensure all systems are running smoothly and communicating with HQ \\n'>\n",
            "<LTTextBoxHorizontal(20) 36.960,304.387,422.731,314.347 'timely and providing recommended solutions rapidly based on the tickets for a faster workflow. \\n'>\n",
            "<LTTextBoxHorizontal(21) 6.600,282.219,104.520,293.259 'TECHNICAL SKILLS \\n'>\n",
            "<LTTextBoxHorizontal(22) 6.480,174.179,599.738,274.982 'Programming Concepts- Python (Anaconda, Data Mining, Natural Language Processing (NLP), Deep Learning, Machine Learning (ML), \\nPredictive  Data  Analytics,  Web  Scraping, Web  Automation,  Selenium,  Beautiful  soup,  TensorFlow,  Pytorch,  BERT),  R studio,  Database \\nsystems (MySQL), Linux (Ubuntu OS), HTML, CSS, Kali, AWS Console, C.  \\nCertifications- Google Data Analytics Professional Certificate, Google Project Management Professional Certificate, AWS Fundamentals Specialization, \\nCode path Cybersecurity Certification \\nManagement Skills: Agile Project Management, Scrum Management, Risk Management, Asana, Jira, Budgeting and Procurement, Waterfall Project Management \\nAnalytics/Productivity Tools- Tableau, Power BI, Google Big Query, Microsoft Suite, Visual Studio, Google Analytics, and Business, Jupyter \\nNotebooks, MariaDB, WordPress \\n'>\n",
            "<LTTextBoxHorizontal(23) 0.960,141.396,183.120,166.539 '  PROJECTS \\n  Project Title: Zillow Home Price Prediction \\n'>\n",
            "<LTTextBoxHorizontal(24) 15.120,107.455,609.455,140.462 'With the dataset of home prices from the years of 2000-2023 and using CRISP-DM methodologies and ML Algorithms such as regression, pattern \\nrecognition,  structured  prediction,  and  cluster  analysis  for  the  data  imputation  to  predict  the  future  home  prices  by  taking  the  inflation  into \\nconsideration. Deployed an ensemble learning model for higher accuracy of the prediction with respect to the Consumer price Index values (CPI). \\n'>\n",
            "<LTTextBoxHorizontal(25) 0.960,90.182,145.171,100.142 '  Project Title: Airline Data Analysis \\n'>\n",
            "<LTTextBoxHorizontal(26) 15.120,55.734,602.025,88.622 'This project aims to utilize Data Mining and Machine Learning algorithms to analyze airports based on the frequency of delays. Additionally, I \\nhave investigated the days of the week that suffer the most delays and the potential causes behind them. Furthermore, I have identified air routes \\nwith unique patterns, such as early departures but late arrivals, and the possible causes behind it. \\n'>\n",
            "<LTTextBoxHorizontal(27) 0.960,38.462,280.887,48.422 '  Project Title: B-Tree Implementation and Automated Data Insertion \\n'>\n",
            "<LTTextBoxHorizontal(28) 15.120,4.014,601.957,36.902 'Built a complete functional and automated B-tree data structure with addition, removal, traversals, and balancing with O(logn) time complexity. \\nThis program is self-capable of recognizing the data from any dataset and automate indexing and converting it to a B-tree for further advanced \\noperations. \\n'>\n",
            "<LTRect 201.000,744.480,325.080,744.960>\n",
            "<LTRect 151.920,732.000,313.320,732.481>\n",
            "<LTRect 356.160,732.000,474.960,732.481>\n",
            "<LTRect 8.880,711.120,132.000,712.200>\n",
            "<LTRect 6.480,602.400,188.880,603.480>\n",
            "<LTRect 6.600,283.560,101.760,284.640>\n",
            "<LTRect 6.240,156.840,62.880,157.920>\n",
            "<LTRect 59.880,143.520,180.120,144.000>\n",
            "<LTRect 58.320,91.680,142.680,92.160>\n",
            "<LTRect 60.960,39.960,278.400,40.440>\n",
            "<LTLine 7.000,730.320,605.000,730.320>\n",
            "<LTTextLineHorizontal 0.960,723.105,2.340,728.625 ' \\n'>\n",
            "<LTTextLineHorizontal 6.960,707.016,7.710,710.016 ' \\n'>\n",
            "<LTTextLineHorizontal 6.480,683.105,6.720,684.065 ' \\n'>\n",
            "<LTTextLineHorizontal 6.480,670.385,6.720,671.345 ' \\n'>\n",
            "<LTTextLineHorizontal 6.480,634.745,6.720,635.705 ' \\n'>\n",
            "<LTTextLineHorizontal 0.960,617.173,2.220,622.213 ' \\n'>\n",
            "<LTTextLineHorizontal 476.640,572.011,479.400,583.051 ' \\n'>\n",
            "<LTTextLineHorizontal 0.960,473.661,1.950,477.621 ' \\n'>\n",
            "<LTTextLineHorizontal 72.960,461.222,75.450,471.182 ' \\n'>\n",
            "<LTTextLineHorizontal 108.955,461.222,111.445,471.182 ' \\n'>\n",
            "<LTTextLineHorizontal 144.950,461.222,147.440,471.182 ' \\n'>\n",
            "<LTTextLineHorizontal 180.946,461.222,183.436,471.182 ' \\n'>\n",
            "<LTTextLineHorizontal 216.941,461.222,219.431,471.182 ' \\n'>\n",
            "<LTTextLineHorizontal 252.937,461.222,255.427,471.182 ' \\n'>\n",
            "<LTTextLineHorizontal 288.932,461.222,291.422,471.182 ' \\n'>\n",
            "<LTTextLineHorizontal 324.928,461.222,327.418,471.182 ' \\n'>\n",
            "<LTTextLineHorizontal 360.923,461.222,363.413,471.182 ' \\n'>\n",
            "<LTTextLineHorizontal 396.918,461.222,399.408,471.182 ' \\n'>\n",
            "<LTTextLineHorizontal 432.914,461.222,435.404,471.182 ' \\n'>\n",
            "<LTTextLineHorizontal 0.960,295.573,2.220,300.613 ' \\n'>\n",
            "<LTTextLineHorizontal 0.960,276.867,2.220,281.907 ' \\n'>\n",
            "<LTTextLineHorizontal 0.960,239.199,1.710,242.199 ' \\n'>\n",
            "<LTTextLineHorizontal 0.960,212.679,1.710,215.679 ' \\n'>\n",
            "<LTTextLineHorizontal 0.960,197.799,1.710,200.799 ' \\n'>\n",
            "<LTTextLineHorizontal 0.960,168.747,2.220,173.787 ' \\n'>\n",
            "<LTTextLineHorizontal 15.120,154.251,15.630,156.291 ' \\n'>\n",
            "<LTTextLineHorizontal 0.960,102.133,2.220,107.173 ' \\n'>\n",
            "<LTTextLineHorizontal 0.960,50.413,2.220,55.453 ' \\n'>\n"
          ]
        }
      ],
      "source": [
        "for page_layout in extract_pages('Manish_Kolla_Resume_Spring24.pdf'):\n",
        "  for element in page_layout:\n",
        "    print(element)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gWKE_pmssn99",
        "outputId": "a9596945-df61-4367-a707-95d2362c8ff1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'str'>\n",
            "Manish Kolla \n",
            "Atlanta, Georgia \n",
            "maneeshkolla2003@gmail.com, +1 (470)-647-9715 \n",
            "\n",
            "LinkedIn: https://www.linkedin.com/in/manishkolla/ | GitHub: https://github.com/manishkolla \n",
            "\n",
            "EDUCATION AND HONORS \n",
            "\n",
            "Georgia State University (Honors College) Atlanta, GA \n",
            "\n",
            "              May 2025 \n",
            "\n",
            "Bachelor of Science in Computer Science, Certificate in Data Science and Cyber Security \n",
            "GPA: 3.82 \n",
            "Awards: President's List (Spring 2022, Fall 2022), Dean's List (Spring 2023, Fall 2021), US Presidential Tuition waiver \n",
            "Relevant Coursework: Data Mining, Fundamentals of Data Science, Data Structures, System Level Programming, Linear Algebra, Calculus, \n",
            "Design Analysis and Algorithms, Computer Organization and Programming, Probability and Statistics \n",
            "Campus Involvement: Indian Student Organization (Executive Board Member), Member of Cybersecurity Club, Math and Stats Club \n",
            "\n",
            "TECHNICAL SKILLS \n",
            "\n",
            "Programming Concepts- Python, Data Mining, Natural Language Processing (NLP), Machine Learning, Artificial Neural Networking, Web \n",
            "Scraping, Web Automation, Data Analytics, Database systems (SQL), R for Statistical Computing, Linux (OS), Ubuntu OS, HTML, CSS, Kali \n",
            "AWS Console, C Programming. Skilled in Microsoft and Linux Operating Systems with a additional expertise in remote systems and servers \n",
            "\n",
            "Certifications- Google Data Analytics Professional Certificate, Google Project Management Professional Certificate, AWS Fundamentals Specialization*, \n",
            "Code path Cybersecurity  \n",
            "\n",
            "Management Skills: Agile Project Management, Scrum Management, Risk Management, Asana, Budgeting and Procurement, Waterfall Project Management \n",
            "\n",
            "Microsoft and Google Suite- Tableau, Google Big Query, Word, Excel, PowerPoint, One Drive, Visual Studio, Google Analytics, Ads, and Business \n",
            "\n",
            "WORK/INTERNSHIP EXPERIENCE   \n",
            "\n",
            "Evidence-Based Cybersecurity Research Group, GSU Atlanta, GA \n",
            "Undergraduate Research Assistant/ Junior Data Scientist \n",
            "\n",
            "September 2022- Present \n",
            "\n",
            "•  Developed and implemented web automation framework to retrieve data from Clearnet, Darknet, and Telegram using Selenium \n",
            "web drivers and debugged several web scrapers which boosted the workflow efficiency by 40% and accuracy up to 90% in \n",
            "scraping of web forums. \n",
            "\n",
            "•  Operating data cleaning and data management techniques to ensure the quality and accuracy of the data for analysis.  \n",
            "•  Used machine learning and data mining approaches to identify patterns and trends in the data and using predictive analysis and \n",
            "training neural networks to forecast the future outcomes and newly evolving catastrophes from the underground markets.  \n",
            "\n",
            "•  Libraries Used: Anaconda, Beautiful soup, Selenium, Machine Learning, Data Mining, Natural Language Processing (NLP), Data \n",
            "Visualization, Dashboards, Data preprocessing, MySQL, MariaDB, JavaScript, and working with Linux (Ubuntu), Jupyter \n",
            "\n",
            "Lavner Education Atlanta, GA  \n",
            "Information Technology Intern \n",
            "\n",
            "                      June 2023- August 2023 \n",
            "\n",
            "•  Maintenance of on-site computer hardware, including desktops, laptops, printers, peripherals, ensuring they are functioning optimally and up \n",
            "\n",
            "to date and managing the inventory of hardware and software assets and troubleshooting them when required. \n",
            "\n",
            "•  Teaching and grading assignments for a class size of 50 high school students in Python, C, Machine Learning and Artificial Intelligence. \n",
            "•  Debugging code, project backup, and computer clean-up to ensure all systems are running smoothly and communicating \n",
            "\n",
            "with HQ timely and providing recommended solutions rapidly based on the tickets for a faster workflow. \n",
            "\n",
            "PROJECTS \n",
            "\n",
            "Project Title: Gold Price Prediction \n",
            "\n",
            "With the dataset of gold prices from the years of 1996-2023 and using Data Mining and Machine Learning Algorithms such as regression, pattern \n",
            "recognition, structured prediction, and cluster analysis to predict the gold prices for the upcoming years. Using Decision tree algorithm to analyze \n",
            "past data and predict future prices. Analyzing the months and years where the transactions and trades are maximum and least among years. \n",
            "\n",
            "Project Title: Airline Data Analysis (Data Mining- CS 4740) \n",
            "\n",
            "This project aims to utilize Data Mining and Machine Learning algorithms to analyze airports based on the frequency of delays. Additionally, I \n",
            "have investigated the days of the week that suffer the most delays and the potential causes behind them. Furthermore, I have identified air routes \n",
            "with unique patterns, such as early departures but late arrivals, and the possible causes behind it.  \n",
            "\n",
            "Project Title: : SPUDOS (HACK GSU'22 and GSU DEMO DAY) \n",
            "\n",
            "I have volunteered as sponsorship coordinator and safety coordinator for the Hackathon. And I have also collaborated with 3 other peers and \n",
            "designed a bus safety application with HTML, JavaScript, CSS, and Google Maps API. \n",
            "\n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "                    \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "\f\n"
          ]
        }
      ],
      "source": [
        "text=extract_text('/content/Manish Kolla- Official Resume23 (2) - handshake.pdf')\n",
        "description= extract_text('/content/RELX Jobs.pdf')\n",
        "print(type(text))\n",
        "print(text)\n",
        "new_text=text.replace('\\n','')\n",
        "description=description.replace('\\n','')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_text"
      ],
      "metadata": {
        "id": "F0sznZQsFgXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0exT25vs-fK",
        "outputId": "9e54b84e-8d23-48a2-f53d-e621290e762b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Manish Kolla \\nAtlanta', ' Georgia \\nmaneeshkolla2003@gmail.com', ' +1 (470)-647-9715 \\n\\nLinkedIn: https://www.linkedin.com/in/manishkolla/ | GitHub: https://github.com/manishkolla \\n\\nEDUCATION AND HONORS \\n\\nGeorgia State University (Honors College) Atlanta', ' GA \\n\\n              May 2025 \\n\\nBachelor of Science in Computer Science', \" Certificate in Data Science and Cyber Security \\nGPA: 3.82 \\nAwards: President's List (Spring 2022\", ' Fall 2022)', \" Dean's List (Spring 2023\", ' Fall 2021)', ' US Presidential Tuition waiver \\nRelevant Coursework: Data Mining', ' Fundamentals of Data Science', ' Data Structures', ' System Level Programming', ' Linear Algebra', ' Calculus', ' \\nDesign Analysis and Algorithms', ' Computer Organization and Programming', ' Probability and Statistics \\nCampus Involvement: Indian Student Organization (Executive Board Member)', ' Member of Cybersecurity Club', ' Math and Stats Club \\n\\nTECHNICAL SKILLS \\n\\nProgramming Concepts- Python', ' Data Mining', ' Natural Language Processing (NLP)', ' Machine Learning', ' Artificial Neural Networking', ' Web \\nScraping', ' Web Automation', ' Data Analytics', ' Database systems (SQL)', ' R for Statistical Computing', ' Linux (OS)', ' Ubuntu OS', ' HTML', ' CSS', ' Kali \\nAWS Console', ' C Programming. Skilled in Microsoft and Linux Operating Systems with a additional expertise in remote systems and servers \\n\\nCertifications- Google Data Analytics Professional Certificate', ' Google Project Management Professional Certificate', ' AWS Fundamentals Specialization*', ' \\nCode path Cybersecurity  \\n\\nManagement Skills: Agile Project Management', ' Scrum Management', ' Risk Management', ' Asana', ' Budgeting and Procurement', ' Waterfall Project Management \\n\\nMicrosoft and Google Suite- Tableau', ' Google Big Query', ' Word', ' Excel', ' PowerPoint', ' One Drive', ' Visual Studio', ' Google Analytics', ' Ads', ' and Business \\n\\nWORK/INTERNSHIP EXPERIENCE   \\n\\nEvidence-Based Cybersecurity Research Group', ' GSU Atlanta', ' GA \\nUndergraduate Research Assistant/ Junior Data Scientist \\n\\nSeptember 2022- Present \\n\\n•  Developed and implemented web automation framework to retrieve data from Clearnet', ' Darknet', ' and Telegram using Selenium \\nweb drivers and debugged several web scrapers which boosted the workflow efficiency by 40% and accuracy up to 90% in \\nscraping of web forums. \\n\\n•  Operating data cleaning and data management techniques to ensure the quality and accuracy of the data for analysis.  \\n•  Used machine learning and data mining approaches to identify patterns and trends in the data and using predictive analysis and \\ntraining neural networks to forecast the future outcomes and newly evolving catastrophes from the underground markets.  \\n\\n•  Libraries Used: Anaconda', ' Beautiful soup', ' Selenium', ' Machine Learning', ' Data Mining', ' Natural Language Processing (NLP)', ' Data \\nVisualization', ' Dashboards', ' Data preprocessing', ' MySQL', ' MariaDB', ' JavaScript', ' and working with Linux (Ubuntu)', ' Jupyter \\n\\nLavner Education Atlanta', ' GA  \\nInformation Technology Intern \\n\\n                      June 2023- August 2023 \\n\\n•  Maintenance of on-site computer hardware', ' including desktops', ' laptops', ' printers', ' peripherals', ' ensuring they are functioning optimally and up \\n\\nto date and managing the inventory of hardware and software assets and troubleshooting them when required. \\n\\n•  Teaching and grading assignments for a class size of 50 high school students in Python', ' C', ' Machine Learning and Artificial Intelligence. \\n•  Debugging code', ' project backup', ' and computer clean-up to ensure all systems are running smoothly and communicating \\n\\nwith HQ timely and providing recommended solutions rapidly based on the tickets for a faster workflow. \\n\\nPROJECTS \\n\\nProject Title: Gold Price Prediction \\n\\nWith the dataset of gold prices from the years of 1996-2023 and using Data Mining and Machine Learning Algorithms such as regression', ' pattern \\nrecognition', ' structured prediction', ' and cluster analysis to predict the gold prices for the upcoming years. Using Decision tree algorithm to analyze \\npast data and predict future prices. Analyzing the months and years where the transactions and trades are maximum and least among years. \\n\\nProject Title: Airline Data Analysis (Data Mining- CS 4740) \\n\\nThis project aims to utilize Data Mining and Machine Learning algorithms to analyze airports based on the frequency of delays. Additionally', ' I \\nhave investigated the days of the week that suffer the most delays and the potential causes behind them. Furthermore', ' I have identified air routes \\nwith unique patterns', ' such as early departures but late arrivals', \" and the possible causes behind it.  \\n\\nProject Title: : SPUDOS (HACK GSU'22 and GSU DEMO DAY) \\n\\nI have volunteered as sponsorship coordinator and safety coordinator for the Hackathon. And I have also collaborated with 3 other peers and \\ndesigned a bus safety application with HTML\", ' JavaScript', ' CSS', ' and Google Maps API. \\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n                    \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\x0c']\n"
          ]
        }
      ],
      "source": [
        "pattern = r\"[^,]+\"\n",
        "skills = re.findall(pattern, text)\n",
        "print(skills)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBctIrIct-9y",
        "outputId": "40a140fa-9a91-43a9-cf66-24921bb6248a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Emails: ['maneeshkolla2003@gmail.com']\n",
            "URLs: ['https://www.linkedin.com/in/manishkolla/', 'https://github.com/manishkolla']\n"
          ]
        }
      ],
      "source": [
        "email_pattern = r\"[\\w\\.-]+@[\\w\\.-]+\\.\\w+\"  # Improved for broader email matching\n",
        "url_pattern =  r'(http|ftp|https):\\/\\/([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:\\/~+#-]*[\\w@?^=%&\\/~+#-])'\n",
        "\n",
        "# Find all matches of the patterns in the text\n",
        "emails = re.findall(email_pattern, text)\n",
        "urls = re.findall(url_pattern, text)\n",
        "combined_urls = []\n",
        "for protocol, domain, path in urls:\n",
        "    combined_urls.append(f\"{protocol}://{domain}{path}\")\n",
        "\n",
        "# Print the extracted emails and URLs\n",
        "print(\"Emails:\", emails)\n",
        "print(\"URLs:\", combined_urls)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyresparser\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AT4lfO7TXxUD",
        "outputId": "089d4d3a-3f72-4ed0-eb6d-0a698e52472e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyresparser\n",
            "  Downloading pyresparser-1.0.6-py3-none-any.whl (4.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.10/dist-packages (from pyresparser) (23.2.0)\n",
            "Requirement already satisfied: blis>=0.2.4 in /usr/local/lib/python3.10/dist-packages (from pyresparser) (0.7.11)\n",
            "Requirement already satisfied: certifi>=2019.6.16 in /usr/local/lib/python3.10/dist-packages (from pyresparser) (2024.2.2)\n",
            "Requirement already satisfied: chardet>=3.0.4 in /usr/local/lib/python3.10/dist-packages (from pyresparser) (5.2.0)\n",
            "Requirement already satisfied: cymem>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from pyresparser) (2.0.8)\n",
            "Collecting docx2txt>=0.7 (from pyresparser)\n",
            "  Downloading docx2txt-0.8.tar.gz (2.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from pyresparser) (3.6)\n",
            "Requirement already satisfied: jsonschema>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from pyresparser) (4.19.2)\n",
            "Requirement already satisfied: nltk>=3.4.3 in /usr/local/lib/python3.10/dist-packages (from pyresparser) (3.8.1)\n",
            "Requirement already satisfied: numpy>=1.16.4 in /usr/local/lib/python3.10/dist-packages (from pyresparser) (1.25.2)\n",
            "Requirement already satisfied: pandas>=0.24.2 in /usr/local/lib/python3.10/dist-packages (from pyresparser) (1.5.3)\n",
            "Requirement already satisfied: pdfminer.six>=20181108 in /usr/local/lib/python3.10/dist-packages (from pyresparser) (20231228)\n",
            "Requirement already satisfied: preshed>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from pyresparser) (3.0.9)\n",
            "Requirement already satisfied: pycryptodome>=3.8.2 in /usr/local/lib/python3.10/dist-packages (from pyresparser) (3.20.0)\n",
            "Collecting pyrsistent>=0.15.2 (from pyresparser)\n",
            "  Downloading pyrsistent-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.7/117.7 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from pyresparser) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2019.1 in /usr/local/lib/python3.10/dist-packages (from pyresparser) (2023.4)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.10/dist-packages (from pyresparser) (2.31.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from pyresparser) (1.16.0)\n",
            "Requirement already satisfied: sortedcontainers>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pyresparser) (2.4.0)\n",
            "Requirement already satisfied: spacy>=2.1.4 in /usr/local/lib/python3.10/dist-packages (from pyresparser) (3.7.2)\n",
            "Requirement already satisfied: srsly>=0.0.7 in /usr/local/lib/python3.10/dist-packages (from pyresparser) (2.4.8)\n",
            "Requirement already satisfied: thinc>=7.0.4 in /usr/local/lib/python3.10/dist-packages (from pyresparser) (8.2.3)\n",
            "Requirement already satisfied: tqdm>=4.32.2 in /usr/local/lib/python3.10/dist-packages (from pyresparser) (4.66.1)\n",
            "Requirement already satisfied: urllib3>=1.25.3 in /usr/local/lib/python3.10/dist-packages (from pyresparser) (2.0.7)\n",
            "Requirement already satisfied: wasabi>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from pyresparser) (1.1.2)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0.1->pyresparser) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0.1->pyresparser) (0.33.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0.1->pyresparser) (0.17.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.4.3->pyresparser) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.4.3->pyresparser) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.4.3->pyresparser) (2023.12.25)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six>=20181108->pyresparser) (3.3.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six>=20181108->pyresparser) (42.0.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from preshed>=2.0.1->pyresparser) (1.0.10)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.1.4->pyresparser) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.1.4->pyresparser) (1.0.5)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.1.4->pyresparser) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.1.4->pyresparser) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.1.4->pyresparser) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.1.4->pyresparser) (6.4.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.1.4->pyresparser) (2.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.1.4->pyresparser) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy>=2.1.4->pyresparser) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.1.4->pyresparser) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.1.4->pyresparser) (3.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc>=7.0.4->pyresparser) (0.1.4)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six>=20181108->pyresparser) (1.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2.1.4->pyresparser) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2.1.4->pyresparser) (2.16.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2.1.4->pyresparser) (4.9.0)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy>=2.1.4->pyresparser) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy>=2.1.4->pyresparser) (2.1.5)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six>=20181108->pyresparser) (2.21)\n",
            "Building wheels for collected packages: docx2txt\n",
            "  Building wheel for docx2txt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docx2txt: filename=docx2txt-0.8-py3-none-any.whl size=3960 sha256=e5f6311e40adc484e6cb868d71e190b6919de0d76077bd3c0003037ce51157bd\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/58/cf/093d0a6c3ecfdfc5f6ddd5524043b88e59a9a199cb02352966\n",
            "Successfully built docx2txt\n",
            "Installing collected packages: docx2txt, pyrsistent, pyresparser\n",
            "Successfully installed docx2txt-0.8 pyresparser-1.0.6 pyrsistent-0.20.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "resume = ' '.join([word for word in text.split() if word.lower() not in stop_words])\n",
        "job=' '.join([word for word in description.split() if word.lower() not in stop_words])"
      ],
      "metadata": {
        "id": "rgzI17sTAsy7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "job"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "M4mZqyNVB3k6",
        "outputId": "e298174a-dd74-4a50-faaf-ddac5b5a3334"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'site, powered Workday, uses cookies essential site work. site may also use additional cookies analyze site statistics HPCC Systems Summer Intern (Remote)optimize site experience. Click “Accept Cookies” accept additional cookies click “Decline” reject non-essential cookies. information, see Cookie Notice.ApplyDeclineAccept CookiesRELX JobsEnglishSign InRELX CareersSearch JobsHPCC Systems Summer Intern (Remote)ApplyRemote - USA - NationwideFull timePosted 30+ Days AgoR69567Simplify2 3 keywordsare present resumeBeta67%resume matchPlease read entire posting including application process belowAbout business:LexisNexis Risk Solutions essential partner assessment risk. Within Technology division, leverage open source super computing platform HPCC Systems process massive amounts data high speed deliver insights beneﬁt people, businesses societies around globe. learn LexisNexis Risk Solutions technology link below.https://risk.lexisnexis.com/our-technologyAbout team: Born deep data analysis experience LexisNexis Risk Solutions, HPCC Systems proven, HPCC Systems Summer Intern (Remote)comprehensive, dedicated cloud-native platform makes combining data stored massive, mixed schema data lakes easier faster. HPCC Systems team composed international group experienced technologists help drive innovation platform support open source community. interested students around globe want become part HPCC Systems community grow career us. learn HPCC Systems community link below.Applyhttps://hpccsystems.com/ job:As HPCC Systems summer intern, part HPCC Systems team period 12 weeks work closely mentor company develop big data project choice using open source technology HPCC Systems. internship experience day-to-day activities real-world Research Development environment acquire important skills future career industry. details internship much covered student wiki page . position remote open US non-US based students (subject conﬁrmation provide country residence application form).ResponsibilitiesLearn technologies tools required successful completion internship project.Under guidance mentor, successfully execute activities internship project plan.Attend daily calls mentor weekly team meetings.Send weekly status/progress report mentor.Maintain public blog post, proper documentation code repository project.At end internship, deliver ﬁnal presentation team produce poster/recording project (see posters/recordings past projects ).Adhere corporate guidelines attend mandatory training required.RequirementsBe student high school PhD levels authorized work country residence.Availability work 40 hrs. per week 12 weeks May’24 August’24.Have project proposal ready submitted time application (see details “Application process” section)Good level proﬁciency spoken written English.A personal computer good internet connection equipped sufﬁcient resources adequately support requirements project.Basic knowledge programming language.Willingness learn HPCC Systems programming language ECL (Enterprise Control Language).Have conceptual knowledge topic internship project pursued.Basic communication organizational skills.Problem analysis, troubleshooting, resolution skills.Ability desire learn new processes technologies.Application process apply internship, need follow three-step process:HPCC Systems Summer Intern (Remote) 1) Select project idea work internship. could idea list publish idea (see list past projects inspire you).Apply2) Write draft project proposal according guidelines share proposal via email mentor listed wiki page project idea selected. working idea, please contact students@hpccsystems .com indicate potential mentor you.3) mentor approves proposal, apply online using button below. Don’t forget attach proposal along CV application process applications submitted without project proposal considered.The deadline applications March 22nd, 2024, 23:59 hrs. ET, wait deadline date make offers students submit excellent proposal early.Are enthusiastic internship us? Get started selecting idea sharing proposal mentor, email students@hpccsystems.com information.At LexisNexis Risk Solutions, diverse employees different perspectives key creating innovative new products global customers. 30 diversity employee networks globally prioritize inclusive leadership equitable processes part culture. aim every employee best version themselves. would actively welcome applications candidates diverse backgrounds underrepresented groups. equal opportunity employer: qualiﬁed applicants considered treated employment without regard race, color, creed, religion, sex, national origin, citizenship status, disability status, protected veteran status, age, marital status, sexual orientation, gender identity, genetic information, characteristic protected law. committed providing fair accessible hiring process. disability need requires accommodation adjustment, please let us know completing Applicant Request Support Form: https://forms.ofﬁce.com/r/eVgFxjLmAK .Please read Candidate Privacy Policy . HPCC Systems Summer Intern (Remote)About UsApplyRELX global provider information analytics professional business customers across industries.We help scientists make new discoveries, lawyers win cases, doctors save lives insurance companies offer customers lower prices. save taxpayers consumers money preventing fraud help executives forge commercial relationships clients.Read MoreFollow Us© 2024 Workday, Inc. rights reserved.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyresparser import ResumeParser\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "data = ResumeParser(\"/content/John Doe Resume.pdf\").get_extracted_data()\n",
        "print(\"Name:\", data[\"name\"])\n",
        "print(\"Email:\", data[\"email\"])\n",
        "print(\"Skills:\", data[\"skills\"])\n",
        "print(\"Degree:\", data[\"degree\"])\n",
        "print(\"Total Experience:\", data[\"total_experience\"])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xm8QGLV1XtA_",
        "outputId": "6129adf0-eca7-4079-eeae-3eefb72eee17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: John Doe\n",
            "Email: john.doe@vikes.csuohio.edu\n",
            "Skills: ['Technical', 'Microsoft excel', 'Project management', 'Communication', 'Excel', 'Research', 'Relationship building', 'Outreach']\n",
            "Degree: ['Bachelor of Arts']\n",
            "Total Experience: 7.75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UM9CLSje_IJN"
      },
      "outputs": [],
      "source": [
        "!pip install nlp"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "# Load Universal Sentence Encoder module\n",
        "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n"
      ],
      "metadata": {
        "id": "t29XBRXW-FDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example paragraphs\n",
        "paragraph1 = text\n",
        "paragraph2 = ['machine learning', 'python', 'r studio', 'natural langauge processing', 'bachelors', 'data mining', 'predictive modeling', 'large datasets', 'intern']\n",
        "\n",
        "# Encode paragraphs\n",
        "embeddings1 = embed([paragraph1])\n",
        "embeddings2 = embed(paragraph2)"
      ],
      "metadata": {
        "id": "XOjrbWBe-3VB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "similarity = cosine_similarity(embeddings1, embeddings2)\n",
        "print(similarity[0][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUmDSPgi-Lvx",
        "outputId": "aea56ce3-d1a5-418e-be99-23acf6c880b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.31909275\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from difflib import SequenceMatcher\n",
        "\n",
        "def find_similar_words(skills, paragraph):\n",
        "    skills = [skill.lower() for skill in skills]\n",
        "    words = re.findall(r'\\w+(?:_\\w+)?', paragraph.lower())\n",
        "\n",
        "    similar_words = []\n",
        "    for word in words:\n",
        "        for skill in skills:\n",
        "            ratio = SequenceMatcher(None, word, skill).ratio()\n",
        "            if ratio > 0.6:\n",
        "                similar_words.append(word)\n",
        "\n",
        "    return set(similar_words)\n",
        "\n",
        "\n",
        "print(find_similar_words(paragraph2, resume))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cwmf8_8Fq1Q",
        "outputId": "a0b8672a-c895-423f-932e-ccdc919e9e47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'intern', 'pattern', 'cleaning', 'printers', 'bachelor', 'internship', 'predictive', 'linear', 'dataset', 'training', 'mining', 'prediction', 'studio', 'learning', 'python', 'machine'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## openai"
      ],
      "metadata": {
        "id": "317KQl71SoHK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install openai==0.28"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0vw7UBFF77x",
        "outputId": "3f31057f-6bb8-4db7-fe26-32b3ed99ca8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai==0.28 in /usr/local/lib/python3.10/dist-packages (0.28.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.9.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2024.2.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (4.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai"
      ],
      "metadata": {
        "id": "Sge6uAgYFyLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "API_KEY= open(\"/content/API_Key.txt\", 'r').read()\n",
        "openai.api_key=API_KEY"
      ],
      "metadata": {
        "id": "rmkRBf0rGvUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are a very helpful assistant who can categorize the text parsed from resume as, technical skills, total experience, overall all job titles, a summary of the resume, typical role the person would be a best fit for?\",\n",
        "        }\n",
        "    ],max_tokens=100)"
      ],
      "metadata": {
        "id": "1p48AVYUICUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log=[]\n",
        "log.append({\"role\": \"system\", \"content\": text})\n",
        "defined_messages= [\"Total work experience of the candiadate?\", \"Technial Skills of the candiadate?\", \"Typical best fit roles for the candiadate?\"]\n",
        "for x in defined_messages[0]:\n",
        "  log.append({\"role\": \"user\", \"content\": x})\n",
        "  response=openai.chat.completions.create(\n",
        "      model='gpt-3.5-turbo',\n",
        "      messages=log\n",
        "  )\n",
        "  answers=response['choices'][0]['message']['content'].strip(\"\\n\").strip()\n",
        "  print(x, \" : \", answers)\n",
        "  log.append({\"role\":\"assistant\", \"content\": answers})\n"
      ],
      "metadata": {
        "id": "bZAHS5KyLFt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log=[]\n",
        "log.append({\"role\": \"system\", \"content\": text})\n",
        "defined_messages= [\"Total work experience of the candiadate?\", \"Technial Skills of the candiadate?\", \"Typical best fit roles for the candiadate?\"]\n",
        "log.append({\"role\": \"user\", \"content\": defined_messages[0]})\n",
        "response=openai.chat.completions.create(\n",
        "    model='tts-1',\n",
        "    messages=log\n",
        "  )\n",
        "answers=response['choices'][0]['message']['content'].strip(\"\\n\").strip()\n",
        "print(x, \" : \", answers)\n",
        "log.append({\"role\":\"assistant\", \"content\": answers})\n"
      ],
      "metadata": {
        "id": "rPIqvK3hP7eb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maRteeNe3wps"
      },
      "source": [
        "# Module-2 for Parsing (Fitz)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_0UwQkM02eW",
        "outputId": "be763367-69d7-4666-f573-b3cf32344d43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'format': 'PDF 1.6', 'title': 'Google Manish Kolla- Resume', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 23 for Word', 'producer': 'Adobe PDF Library 23.6.156', 'creationDate': \"D:20231207103322-05'00'\", 'modDate': \"D:20231207103324-05'00'\", 'trapped': '', 'encryption': None}\n"
          ]
        }
      ],
      "source": [
        "import fitz\n",
        "doc=fitz.open('Manish_Kolla_Resume_Fall23_DM.pdf')\n",
        "print(doc.metadata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PuUTZERo3DjU",
        "outputId": "02de974c-e92d-4517-de1d-7f47985ef1c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Manish Kolla \n",
            "Atlanta, Georgia \n",
            "maneeshkolla2003@gmail.com, +1 (470)-647-9715 \n",
            "\n",
            "LinkedIn: https://www.linkedin.com/in/manishkolla/ | GitHub: https://github.com/manishkolla \n",
            "\n",
            " EDUCATION AND HONORS \n",
            " Georgia State University (Honors College) Atlanta, GA \n",
            " Bachelor of Science in Computer Science, Certificate in Data Science \n",
            " GPA: 3.82 \n",
            "Awards: President's List (Spring 2022, Fall 2022), Dean's List (Spring 2023, Fall 2021), US Presidential Tuition waiver \n",
            "Relevant Coursework: Data Mining, Fundamentals of Data Science, Data Structures, System Level Programming, Design Analysis and \n",
            "Algorithms, Computer Organization and Programming, Software Engineering, Programming Language Concepts, Machine Learning, Linear \n",
            "Algebra, Probability and Statistics, Database Systems.  \n",
            "Campus Involvement: Indian Student Organization (Vice President), Association for Computing Machinery, Math and Stats Club \n",
            "\n",
            "May 2025 \n",
            "\n",
            "TECHNICAL SKILLS \n",
            "\n",
            "Programming Concepts- Python, Data Mining, Natural Language Processing (NLP), Machine Learning, Artificial Neural Networking, Web \n",
            "Scraping,  Web  Automation,  R for Statistical Computing,  Database  systems (SQL), Data  Analytics,  Linux  (OS), Ubuntu  OS,  HTML,  Kali, \n",
            "AWS Console, C. Skilled in Microsoft and Linux Operating Systems with additional expertise in remote systems and servers. \n",
            "Certifications- Google Data Analytics Professional Certificate, Google Project Management Professional Certificate, AWS Fundamentals Specialization*, \n",
            "Code path Cybersecurity Certification \n",
            "Management Skills: Agile Project Management, Scrum Management, Risk Management, Asana, Jira, Budgeting and Procurement, Waterfall Project Management \n",
            "Analytics/Productivity Tools- Tableau, MongoDB, MariaDB Google Big Query, Word, Excel, PowerPoint, OneDrive, Visual Studio, Google Analytics, \n",
            "and Business, Jupyter Notebooks \n",
            "\n",
            "  WORK/INTERNSHIP EXPERIENCE \n",
            "\n",
            "  Undergraduate Research Analyst/ Data Scientist \n",
            "  Evidence Based Cybersecurity Research Group (EBCS), Atlanta, GA \n",
            "\n",
            " September 2022- Present \n",
            "\n",
            "•  Developed and implemented web automation framework to retrieve data from Clearnet, Darknet, and Telegram using Selenium \n",
            "web drivers and debugged several web scrapers which boosted the workflow efficiency by 40% and accuracy up to 90% in \n",
            "scraping of web forums. Using CRISP-DM and ETL approaches for Data scraping and reporting the results to sponsors. \n",
            "\n",
            "•  Operating data cleaning and data management techniques to ensure the quality and accuracy of the data for analysis. \n",
            "•  Used machine learning and data mining approaches to identify patterns and trends in the data and using predictive analysis and \n",
            "training neural networks to forecast the future outcomes and newly evolving catastrophes from the underground markets. \n",
            "\n",
            "•  Libraries Used: Anaconda, Beautiful soup, Selenium, Machine Learning, Data Mining, Natural Language Processing (NLP), Data \n",
            "Visualization, Dashboards, Data preprocessing, MySQL, MariaDB, JavaScript, and working with Linux (Ubuntu), Jupyter \n",
            "\n",
            "      STEM Tutor   \n",
            "      Department of Mathematics and Statistics, GSU, Atlanta, GA \n",
            "\n",
            "       January 2022- Present \n",
            "\n",
            "•  Assisting students with a class size of 40-50 on various kinds of math doubts and teaching them the topics they have difficulty understanding.  \n",
            "• \n",
            "• \n",
            "\n",
            "Interacting with professor during the lecture halls and proctored during examinations organizing quizzes in absence of professors. \n",
            "Interacting with the professor regularly and getting updates on the topic’s students have difficulty in understanding and organizing group \n",
            "study sessions for those students. Tutored more than 500 happy students and received “Best Tutor Award for Academic Year Fall 2023.”  \n",
            "\n",
            "   Information Technology Intern \n",
            "   Lavner Education, Atlanta, GA \n",
            "\n",
            " June 2023- August 2023 \n",
            "\n",
            "•  Maintenance of on-site computer hardware, including desktops, laptops, printers, peripherals, ensuring they are functioning optimally and up \n",
            "\n",
            "to date and managed the inventory of hardware and software assets and troubleshooting them when required. \n",
            "\n",
            "•  Teaching and grading assignments for a class size of 50 middle/high school students in Python, C, Machine Learning and Artificial Intelligence. \n",
            "•  Debugging code, project backup, and computer clean-up to ensure all systems are running smoothly and communicating \n",
            "\n",
            "with HQ timely and providing recommended solutions rapidly based on the tickets for a faster workflow. \n",
            "\n",
            "PROJECTS \n",
            "Project Title: Zillow Home Price Prediction \n",
            "With the dataset of home prices from the years of 2000-2023 and using Data Mining and ML Algorithms such as regression, pattern recognition, \n",
            "structured prediction, and cluster analysis for the data imputation to predict the upcoming home prices by taking the inflation into consideration. \n",
            "Using Random Forest algorithm to analyze past data and predict future prices. Using various data manipulation to decrease the error.  \n",
            "Project Title: Airline Data Analysis (Data Mining- CS 4740) \n",
            "This project aims to utilize Data Mining and Machine Learning algorithms to analyze airports based on the frequency of delays. Additionally, I \n",
            "have investigated the days of the week that suffer the most delays and the potential causes behind them. Furthermore, I have identified air routes \n",
            "with unique patterns, such as early departures but late arrivals, and the possible causes behind it. \n",
            "\n",
            "Project Title: : SPUDOS (HACK GSU'22 and GSU DEMO DAY) \n",
            "Have volunteered as sponsorship coordinator and safety coordinator for the Hackathon. And I have also collaborated with 3 other peers and \n",
            "designed a bus safety application with HTML, JavaScript, CSS, and Google Maps API. \n",
            "\n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "\f\n"
          ]
        }
      ],
      "source": [
        "page=doc.load_page(0)\n",
        "text_2=page.get_text()\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PycUMcp2eb4",
        "outputId": "56a811a8-3167-43ea-f40d-a0ede2c375c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'kind': 2, 'xref': 100, 'from': Rect(198.75, 36.56402587890625, 329.72698974609375, 49.97998046875), 'uri': 'mailto:maneeshkolla2003@gmail.com', 'id': ''}, {'kind': 2, 'xref': 98, 'from': Rect(168.13299560546875, 49.97998046875, 187.38900756835938, 60.47998046875), 'uri': 'http://www.linkedin.com/in/manishkolla/', 'id': ''}, {'kind': 2, 'xref': 96, 'from': Rect(199.23300170898438, 49.97998046875, 315.6199951171875, 60.47998046875), 'uri': 'http://www.linkedin.com/in/manishkolla/', 'id': ''}, {'kind': 2, 'xref': 94, 'from': Rect(65.58399963378906, 700.4000854492188, 154.33399963378906, 711.899169921875), 'uri': 'https://github.com/manishkolla/Airline-Data-Analysis', 'id': ''}, {'kind': 2, 'xref': 92, 'from': Rect(71.36209869384766, 749.8458862304688, 113.61100006103516, 761.344970703125), 'uri': 'https://devpost.com/software/spudos?ref_content=my-projects-tab&ref_feature=my_projects', 'id': ''}]\n"
          ]
        }
      ],
      "source": [
        "print(page.get_links())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "317KQl71SoHK",
        "maRteeNe3wps"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}